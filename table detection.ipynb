{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11545293,"sourceType":"datasetVersion","datasetId":7235910},{"sourceId":12125502,"sourceType":"datasetVersion","datasetId":7635119},{"sourceId":12157511,"sourceType":"datasetVersion","datasetId":7656855}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q pytorch-lightning","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torchvision\nimport os\n\nclass CocoDetection(torchvision.datasets.CocoDetection):\n    def __init__(self, img_folder, processor, train=True):\n        ann_file = os.path.join(img_folder, \"train_data-1 (1).json\" if train else \"test_data-1 (1).json\")\n        super(CocoDetection, self).__init__(img_folder, ann_file)\n        self.processor = processor\n\n    def __getitem__(self, idx):\n        # read in PIL image and target in COCO format\n        # feel free to add data augmentation here before passing them to the next step\n        img, target = super(CocoDetection, self).__getitem__(idx)\n\n        # preprocess image and target (converting target to DETR format, resizing + normalization of both image and target)\n        image_id = self.ids[idx]\n        target = {'image_id': image_id, 'annotations': target}\n        encoding = self.processor(images=img, annotations=target, return_tensors=\"pt\")\n        pixel_values = encoding[\"pixel_values\"].squeeze() # remove batch dimension\n        target = encoding[\"labels\"][0] # remove batch dimension\n\n        return pixel_values, target","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from transformers import DetrImageProcessor\nfrom transformers import AutoImageProcessor\n\n# processor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\n# processor = AutoImageProcessor.from_pretrained(\"microsoft/table-transformer-structure-recognition\")\nprocessor = AutoImageProcessor.from_pretrained(\"microsoft/table-transformer-detection\")\nprocessor.size['shortest_edge'] = 800\ntrain_dataset = CocoDetection(img_folder='/kaggle/input/table-structure-recognition/train_task02/', processor=processor)\nval_dataset = CocoDetection(img_folder='/kaggle/input/table-structure-recognition/val_task02/', processor=processor, train=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Number of training examples:\", len(train_dataset))\nprint(\"Number of validation examples:\", len(val_dataset))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport os\nfrom IPython.display import display\nfrom PIL import Image, ImageDraw\n\n# Lấy danh sách các ID ảnh từ COCO dataset\nimage_ids = train_dataset.coco.getImgIds()\n\n# Chọn một ảnh ngẫu nhiên\nimage_id = image_ids[np.random.randint(0, len(image_ids))]\nprint('Image n°{}'.format(image_id))\n\n# Load thông tin ảnh và ảnh từ thư mục\nimage_info = train_dataset.coco.loadImgs(image_id)[0]\nimage_path = os.path.join('/kaggle/input/table-detection/train_task01/', image_info['file_name'])\nimage = Image.open(image_path).convert(\"RGB\")\n\n# Lấy annotation cho ảnh đó\nannotations = train_dataset.coco.imgToAnns[image_id]\n\n# Tạo đối tượng để vẽ\ndraw = ImageDraw.Draw(image, \"RGBA\")\n\n# Mapping từ category_id sang tên nhãn\ncats = train_dataset.coco.cats\nid2label = {k: v['name'] for k, v in cats.items()}\n\n#if 0 not in id2label:\n    #id2label[0] = \"row\" \n\n# Vẽ bbox và nhãn lên ảnh\nfor annotation in annotations:\n    box = annotation['bbox']  # bbox dạng [x, y, width, height]\n    class_idx = annotation['category_id']\n    \n    x, y, w, h = map(int, box)  # Chuyển bbox sang số nguyên\n    draw.rectangle((x, y, x + w, y + h), outline='red', width=2)\n    \n    label_text = id2label.get(class_idx, f\"class_{class_idx}\")  # Nếu không tìm thấy thì dùng tên mặc định\n    draw.text((x, y), label_text, fill='white')\n\n# Hiển thị ảnh kết quả\ndisplay(image)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Lấy danh sách các ID ảnh từ dataset COCO\nimage_ids = train_dataset.ids  # hoặc train_dataset.coco.getImgIds()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(id2label)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\ndef collate_fn(batch):\n  pixel_values = [item[0] for item in batch]\n  encoding = processor.pad(pixel_values, return_tensors=\"pt\")\n  labels = [item[1] for item in batch]\n  batch = {}\n  batch['pixel_values'] = encoding['pixel_values']\n  batch['pixel_mask'] = encoding['pixel_mask']\n  batch['labels'] = labels\n  return batch\n\ntrain_dataloader = DataLoader(train_dataset, collate_fn=collate_fn, batch_size=4, shuffle=True)\nval_dataloader = DataLoader(val_dataset, collate_fn=collate_fn, batch_size=2)\nbatch = next(iter(train_dataloader))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"batch.keys()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pixel_values, target = train_dataset[0]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pixel_values.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(target)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pytorch_lightning as pl\n# from transformers import DetrForObjectDetection\nfrom transformers import AutoModelForObjectDetection\nimport torch\n\nclass Detr(pl.LightningModule):\n     def __init__(self, lr, lr_backbone, weight_decay):\n         super().__init__()\n         # replace COCO classification head with custom head\n         # we specify the \"no_timm\" variant here to not rely on the timm library\n         # for the convolutional backbone\n        #  self.model = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\",\n        #                                                      revision=\"no_timm\",\n        #                                                      num_labels=len(id2label),\n        #                                                      ignore_mismatched_sizes=True)\n         self.model = AutoModelForObjectDetection.from_pretrained(\"microsoft/table-transformer-detection\",\n                                                                        # revision=\"no_timm\",\n                                                                        num_labels=len(id2label),\n                                                                        ignore_mismatched_sizes=True)\n         # see https://github.com/PyTorchLightning/pytorch-lightning/pull/1896\n         self.lr = lr\n         self.lr_backbone = lr_backbone\n         self.weight_decay = weight_decay\n\n     def forward(self, pixel_values, pixel_mask):\n       outputs = self.model(pixel_values=pixel_values, pixel_mask=pixel_mask)\n\n       return outputs\n\n     def common_step(self, batch, batch_idx):\n       pixel_values = batch[\"pixel_values\"]\n       pixel_mask = batch[\"pixel_mask\"]\n       labels = [{k: v.to(self.device) for k, v in t.items()} for t in batch[\"labels\"]]\n\n       outputs = self.model(pixel_values=pixel_values, pixel_mask=pixel_mask, labels=labels)\n\n       loss = outputs.loss\n       loss_dict = outputs.loss_dict\n\n       return loss, loss_dict\n\n     def training_step(self, batch, batch_idx):\n        loss, loss_dict = self.common_step(batch, batch_idx)\n        # logs metrics for each training_step,\n        # and the average across the epoch\n        self.log(\"training_loss\", loss)\n        for k,v in loss_dict.items():\n          self.log(\"train_\" + k, v.item())\n\n        return loss\n\n     def validation_step(self, batch, batch_idx):\n        loss, loss_dict = self.common_step(batch, batch_idx)\n        self.log(\"validation_loss\", loss)\n        for k,v in loss_dict.items():\n          self.log(\"validation_\" + k, v.item())\n\n        return loss\n\n     def configure_optimizers(self):\n        param_dicts = [\n              {\"params\": [p for n, p in self.named_parameters() if \"backbone\" not in n and p.requires_grad]},\n              {\n                  \"params\": [p for n, p in self.named_parameters() if \"backbone\" in n and p.requires_grad],\n                  \"lr\": self.lr_backbone,\n              },\n        ]\n        optimizer = torch.optim.AdamW(param_dicts, lr=self.lr,\n                                  weight_decay=self.weight_decay)\n\n        return optimizer\n\n     def train_dataloader(self):\n        return train_dataloader\n\n     def val_dataloader(self):\n        return val_dataloader","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = Detr(lr=1e-4, lr_backbone=1e-5, weight_decay=1e-4)\n\noutputs = model(pixel_values=batch['pixel_values'], pixel_mask=batch['pixel_mask'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"outputs.logits.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"learnable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f\"Số tham số học được: {learnable_params}/{total_params}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pytorch_lightning import Trainer\n\ntrainer = Trainer(max_epochs=200, gradient_clip_val=0.1, accelerator='gpu', devices=1)\ntrainer.fit(model)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.model.push_to_hub(\"10Ngoc/task02update\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pytorch_lightning import Trainer\nfrom pytorch_lightning.loggers import CSVLogger\nimport pandas as pd\nimport matplotlib.pyplot as plt","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom pytorch_lightning import Trainer\nfrom pytorch_lightning.loggers import CSVLogger\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Tạo mô hình với các siêu tham số\nmodel = Detr(lr=1e-4, lr_backbone=1e-5, weight_decay=1e-4)\noutputs = model(pixel_values=batch['pixel_values'], pixel_mask=batch['pixel_mask'])\n\n# Tạo logger để lưu loss mỗi epoch\nlogger = CSVLogger(save_dir=\"logs/\", name=\"detr_structure_recognition\")\n\n# Cấu hình Trainer\ntrainer = Trainer(\n    max_epochs=10,\n    logger=logger,\n    accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n    log_every_n_steps=10,\n)\n\n# Huấn luyện mô hình\ntrainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n\n# Đọc log CSV để vẽ biểu đồ\nlog_path = logger.log_dir + \"/metrics.csv\"\ndf = pd.read_csv(log_path)\n\n# Vẽ biểu đồ loss\nplt.plot(df[\"epoch\"], df[\"training_loss\"], label=\"Train Loss\")\nif \"validation_loss\" in df.columns:\n    plt.plot(df[\"epoch\"], df[\"validation_loss\"], label=\"Val Loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.title(\"Training & Validation Loss\")\nplt.legend()\nplt.grid(True)\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoImageProcessor, AutoModelForObjectDetection, TableTransformerForObjectDetection\nimport torch\n\nmodel = TableTransformerForObjectDetection.from_pretrained(\"10Ngoc/task01tabledetection\", id2label={0:\"table\"})\n# model = TableTransformerForObjectDetection.from_pretrained(\"microsoft/table-transformer-detection\",\n#                                                                         revision=\"no_timm\",\n#                                                                         num_labels=len(id2label),\n#                                                                         ignore_mismatched_sizes=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q coco-eval","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def convert_to_xywh(boxes):\n    xmin, ymin, xmax, ymax = boxes.unbind(1)\n    return torch.stack((xmin, ymin, xmax - xmin, ymax - ymin), dim=1)\n\ndef prepare_for_coco_detection(predictions):\n    coco_results = []\n    for original_id, prediction in predictions.items():\n        if len(prediction) == 0:\n            continue\n\n        boxes = prediction[\"boxes\"]\n        boxes = convert_to_xywh(boxes).tolist()\n        scores = prediction[\"scores\"].tolist()\n        labels = prediction[\"labels\"].tolist()\n\n        coco_results.extend(\n            [\n                {\n                    \"image_id\": original_id,\n                    \"category_id\": labels[k],\n                    \"bbox\": box,\n                    \"score\": scores[k],\n                }\n                for k, box in enumerate(boxes)\n            ]\n        )\n    return coco_results","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from coco_eval import CocoEvaluator\nfrom tqdm.notebook import tqdm\n\nimport numpy as np\n\n# initialize evaluator with ground truth (gt)\nevaluator = CocoEvaluator(coco_gt=train_dataset.coco, iou_types=[\"bbox\"])\n\nprint(\"Running evaluation...\")\nfor idx, batch in enumerate(tqdm(train_dataloader)):\n    # get the inputs\n    pixel_values = batch[\"pixel_values\"].to(device)\n    pixel_mask = batch[\"pixel_mask\"].to(device)\n    labels = [{k: v.to(device) for k, v in t.items()} for t in batch[\"labels\"]] # these are in DETR format, resized + normalized\n\n    # forward pass\n    with torch.no_grad():\n      outputs = model(pixel_values=pixel_values, pixel_mask=pixel_mask)\n\n    # turn into a list of dictionaries (one item for each example in the batch)\n    orig_target_sizes = torch.stack([target[\"orig_size\"] for target in labels], dim=0)\n    results = processor.post_process_object_detection(outputs, target_sizes=orig_target_sizes, threshold=0)\n\n    # provide to metric\n    # metric expects a list of dictionaries, each item\n    # containing image_id, category_id, bbox and score keys\n    predictions = {target['image_id'].item(): output for target, output in zip(labels, results)}\n    predictions = prepare_for_coco_detection(predictions)\n    evaluator.update(predictions)\n\nevaluator.synchronize_between_processes()\nevaluator.accumulate()\nevaluator.summarize()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import requests\nfrom io import BytesIO\nfrom PIL import Image\n\nurl = \"https://www.dropbox.com/scl/fi/glyymn5opvhmd929004ri/28_1.jpg?rlkey=d99hbhszy1z922ywei315bl06&st=ewtu61ki&dl=1\"\n\n# Dropbox link gốc khi tải trực tiếp, thường bạn phải đổi 'dl=0' thành 'dl=1' hoặc 'raw=1' để có link trực tiếp\n# Ở đây dl=1 rồi nên tải trực tiếp được.\n\nresponse = requests.get(url)\nimage = Image.open(BytesIO(response.content)).convert(\"RGB\")\n\n# Tiếp theo, như bạn đã làm:\ninputs = processor(images=image, return_tensors=\"pt\").to(device)\n\nwith torch.no_grad():\n    outputs = model(**inputs)\n\ntarget_sizes = torch.tensor([image.size[::-1]]).to(device)\nresults = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.5)[0]\n\npredictions = [{\n    \"image_id\": 0,\n    \"category_id\": label.item(),\n    \"bbox\": box.tolist(),\n    \"score\": score.item()\n} for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"])]\n\n\nfrom PIL import Image\nimport itertools, math\nimport matplotlib.pyplot as plt\n\n# ------------------------------------------------\n# 0. Thông số\n# ------------------------------------------------\nPAD = 1                            # số pixel padding quanh mỗi cell\n\n# ------------------------------------------------\n# 1. Phân loại bbox hàng / cột\n# ------------------------------------------------\nrows = [tuple(map(int, p[\"bbox\"])) for p in predictions if p[\"category_id\"] == 0]\ncols = [tuple(map(int, p[\"bbox\"])) for p in predictions if p[\"category_id\"] == 1]\n\n# ------------------------------------------------\n# 2. Giao nhau -> box ô\n# ------------------------------------------------\ndef intersect(a, b):\n    x0, y0 = max(a[0], b[0]), max(a[1], b[1])\n    x1, y1 = min(a[2], b[2]), min(a[3], b[3])\n    return (x0, y0, x1, y1) if x1 > x0 and y1 > y0 else None\n\ncells = [intersect(r, c) for r, c in itertools.product(rows, cols)]\ncells = [b for b in cells if b]                     # loại ô ảo\ncells.sort(key=lambda b: (b[1], b[0]))              # đọc trên-xuống, trái-phải\n\n# ------------------------------------------------\n# 3. Hàm pad & cắt\n# ------------------------------------------------\nW, H = image.size\ndef pad_box(b, p=PAD):\n    x0 = max(0, b[0] - p)\n    y0 = max(0, b[1] - p)\n    x1 = min(W, b[2] + p)\n    y1 = min(H, b[3] + p)\n    return (x0, y0, x1, y1)\n\ncell_imgs = [image.crop(pad_box(b)) for b in cells]\n\n# ------------------------------------------------\n# 4. Hiển thị lưới (tùy chọn)\n# ------------------------------------------------\nn = len(cell_imgs)\ncols_grid = min(6, n)\nrows_grid = math.ceil(n / cols_grid)\nfig, axes = plt.subplots(rows_grid, cols_grid, figsize=(2.5*cols_grid, 2.5*rows_grid))\n\nfor ax in axes.flat:\n    ax.axis(\"off\")\nfor i, im in enumerate(cell_imgs):\n    ax = axes.flat[i]\n    ax.imshow(im)\n    ax.set_title(f\"Cell {i}\", fontsize=8)\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import requests, zipfile, io, os, itertools, math, shutil\nfrom pathlib import Path\nfrom PIL import Image, ImageOps\nimport torch\n\n\n# xóa ZIP cũ nếu có\nif Path(OUTPUT_ZIP).exists():\n    Path(OUTPUT_ZIP).unlink()\n\n# ────────────────────────────────────────────────────────────────────\n# CONFIGURATION (customize chỗ này nha)\n# ────────────────────────────────────────────────────────────────────\nZIP_URL       = \"https://www.dropbox.com/scl  /fo/ovyizztnavkpf3929nu2g/AId_MJ7X-E4XcV96jMIT3yI?rlkey=6i37jm6nr4jsx5z3d2f1fwsze&dl=1\"\nTMP_DIR       = Path(\"ZIP_EXTR\")\nOUT_ROOT      = Path(\"OUT_CELLS\")\nPAD           = 1                  # pixel padding around each cell crop\nDEVICE        = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n# dọn sạch mọi thứ của lần chạy trước\nfor p in (TMP_DIR, OUT_ROOT):\n    if p.exists():\n        shutil.rmtree(p)\n    p.mkdir(parents=True, exist_ok=True)\n\n# resize rules\nSQUARE_SIZE   = (512, 512)\nRECT_SIZE     = (640, 480)\nASPECT_TOLER  = 0.05               # |w/h − 1| ≤ tol → treat as square\nKEEP_ASPECT   = False # Stretch the image\nPAD_COLOR     = (255, 255, 255)    # for letter-box\n\n# name of the final zip you will download\nOUTPUT_ZIP    = \"OUT_CELLS.zip\"\n\n# ────────────────────────────────────────────────────────────────────\n# HOUSEKEEPING  (clean old runs)\n# ────────────────────────────────────────────────────────────────────\nshutil.rmtree(TMP_DIR,  ignore_errors=True)\nshutil.rmtree(OUT_ROOT, ignore_errors=True)\nfor p in (TMP_DIR, OUT_ROOT):\n    p.mkdir(parents=True, exist_ok=True)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n# ────────────────────────────────────────────────────────────────────\n# 1. DOWNLOAD ZIP\n# ────────────────────────────────────────────────────────────────────\nprint(\"⬇️  Downloading ZIP …\")\nresp = requests.get(ZIP_URL, stream=True)\nresp.raise_for_status()\nzip_bytes = io.BytesIO(resp.content)\n\n# ────────────────────────────────────────────────────────────────────\n# 2. EXTRACT JPGs\n# ────────────────────────────────────────────────────────────────────\nprint(\"📦 Extracting JPGs …\")\nwith zipfile.ZipFile(zip_bytes) as zf:\n    for member in zf.namelist():\n        if member.lower().endswith(\".jpg\"):\n            zf.extract(member, path=TMP_DIR)\n\njpg_paths = sorted(TMP_DIR.rglob(\"*.jpg\"))\nprint(f\"✅ Found {len(jpg_paths)} images\\n\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Utils functions","metadata":{}},{"cell_type":"code","source":"def intersect(a, b):\n    x0, y0 = max(a[0], b[0]), max(a[1], b[1])\n    x1, y1 = min(a[2], b[2]), min(a[3], b[3])\n    return (x0, y0, x1, y1) if x1 > x0 and y1 > y0 else None\n\ndef pad_box(b, p, w, h):\n    x0 = max(0, b[0]-p); y0 = max(0, b[1]-p)\n    x1 = min(w, b[2]+p); y1 = min(h, b[3]+p)\n    return (x0, y0, x1, y1)\n\ndef resize_with_letterbox(img, target, keep_aspect=True, fill=PAD_COLOR):\n    \"\"\"Resize to *target* (w,h).  If keep_aspect→True, letter-box pad.\"\"\"\n    if not keep_aspect:\n        return img.resize(target, Image.LANCZOS)\n\n\n    tw, th = target\n    iw, ih = img.size\n    scale  = min(tw/iw, th/ih)\n    nw, nh = int(iw*scale), int(ih*scale)\n    img_r  = img.resize((nw, nh), Image.LANCZOS)\n    canvas = Image.new(\"RGB\", target, fill)\n    canvas.paste(img_r, ((tw-nw)//2, (th-nh)//2))\n    return canvas\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Process each image","metadata":{}},{"cell_type":"code","source":"\nfor img_path in jpg_paths:\n    image = Image.open(img_path).convert(\"RGB\")\n\n    # ── Run detector ────────────────────────────────────────────────\n    inputs = processor(images=image, return_tensors=\"pt\").to(DEVICE)\n    with torch.no_grad():\n        outputs = model(**inputs)\n\n    target_sizes = torch.tensor([image.size[::-1]]).to(DEVICE)\n    results = processor.post_process_object_detection(\n                    outputs, target_sizes=target_sizes, threshold=0.5)[0]\n\n    predictions = [{\n        \"category_id\": label.item(),\n        \"bbox\": list(map(int, box.tolist()))\n    } for label, box in zip(results[\"labels\"], results[\"boxes\"])]\n\n    rows = [p[\"bbox\"] for p in predictions if p[\"category_id\"] == 0]\n    cols = [p[\"bbox\"] for p in predictions if p[\"category_id\"] == 1]\n\n    cell_boxes = [intersect(r, c) for r, c in itertools.product(rows, cols)]\n    cell_boxes = [b for b in cell_boxes if b]\n    cell_boxes.sort(key=lambda b: (b[1], b[0]))   # reading order\n\n    # ── Crop, resize, save ──────────────────────────────────────────\n    W, H = image.size\n    out_dir = OUT_ROOT / img_path.stem\n    out_dir.mkdir(parents=True, exist_ok=True)\n\n    for idx, b in enumerate(cell_boxes):\n        pb   = pad_box(b, PAD, W, H)\n        crop = image.crop(pb)\n\n        # choose target size\n        w, h      = crop.size\n        is_square = abs((w/h) - 1.0) <= ASPECT_TOLER\n        target_sz = SQUARE_SIZE if is_square else RECT_SIZE\n        \n        final = resize_with_letterbox(\n                    crop,\n                    target=target_sz,\n                    keep_aspect=False,        # <── tắt giữ tỉ lệ\n                    fill=PAD_COLOR            # giá trị này sẽ bị bỏ qua khi keep_aspect=False\n                )\n        final.save(out_dir / f\"cell_{idx:03d}.jpg\", quality=95)\n\n    print(f\"{img_path.name:35s} ➜  {len(cell_boxes):3d} cells\")\n\nprint(\"\\n🥳  All done! Crops live in:\", OUT_ROOT.resolve())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Make zip file","metadata":{}},{"cell_type":"code","source":"\nif os.path.exists(OUTPUT_ZIP):\n    os.remove(OUTPUT_ZIP)\n\nshutil.make_archive(OUTPUT_ZIP.replace(\".zip\",\"\"), \"zip\", root_dir=OUT_ROOT)\nprint(f\"📁  Created {OUTPUT_ZIP} – grab it from the right-hand Files pane.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import FileLink, display\ndisplay(FileLink(OUTPUT_ZIP))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}