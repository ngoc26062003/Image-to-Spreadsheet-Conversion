{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11545293,"sourceType":"datasetVersion","datasetId":7235910},{"sourceId":12125502,"sourceType":"datasetVersion","datasetId":7635119},{"sourceId":12157511,"sourceType":"datasetVersion","datasetId":7656855}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q pytorch-lightning","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torchvision\nimport os\n\nclass CocoDetection(torchvision.datasets.CocoDetection):\n    def __init__(self, img_folder, processor, train=True):\n        ann_file = os.path.join(img_folder, \"train_data-1 (1).json\" if train else \"test_data-1 (1).json\")\n        super(CocoDetection, self).__init__(img_folder, ann_file)\n        self.processor = processor\n\n    def __getitem__(self, idx):\n        # read in PIL image and target in COCO format\n        # feel free to add data augmentation here before passing them to the next step\n        img, target = super(CocoDetection, self).__getitem__(idx)\n\n        # preprocess image and target (converting target to DETR format, resizing + normalization of both image and target)\n        image_id = self.ids[idx]\n        target = {'image_id': image_id, 'annotations': target}\n        encoding = self.processor(images=img, annotations=target, return_tensors=\"pt\")\n        pixel_values = encoding[\"pixel_values\"].squeeze() # remove batch dimension\n        target = encoding[\"labels\"][0] # remove batch dimension\n\n        return pixel_values, target","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from transformers import DetrImageProcessor\nfrom transformers import AutoImageProcessor\n\n# processor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\n# processor = AutoImageProcessor.from_pretrained(\"microsoft/table-transformer-structure-recognition\")\nprocessor = AutoImageProcessor.from_pretrained(\"microsoft/table-transformer-detection\")\nprocessor.size['shortest_edge'] = 800\ntrain_dataset = CocoDetection(img_folder='/kaggle/input/table-structure-recognition/train_task02/', processor=processor)\nval_dataset = CocoDetection(img_folder='/kaggle/input/table-structure-recognition/val_task02/', processor=processor, train=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Number of training examples:\", len(train_dataset))\nprint(\"Number of validation examples:\", len(val_dataset))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport os\nfrom IPython.display import display\nfrom PIL import Image, ImageDraw\n\n# Láº¥y danh sÃ¡ch cÃ¡c ID áº£nh tá»« COCO dataset\nimage_ids = train_dataset.coco.getImgIds()\n\n# Chá»n má»™t áº£nh ngáº«u nhiÃªn\nimage_id = image_ids[np.random.randint(0, len(image_ids))]\nprint('Image nÂ°{}'.format(image_id))\n\n# Load thÃ´ng tin áº£nh vÃ  áº£nh tá»« thÆ° má»¥c\nimage_info = train_dataset.coco.loadImgs(image_id)[0]\nimage_path = os.path.join('/kaggle/input/table-detection/train_task01/', image_info['file_name'])\nimage = Image.open(image_path).convert(\"RGB\")\n\n# Láº¥y annotation cho áº£nh Ä‘Ã³\nannotations = train_dataset.coco.imgToAnns[image_id]\n\n# Táº¡o Ä‘á»‘i tÆ°á»£ng Ä‘á»ƒ váº½\ndraw = ImageDraw.Draw(image, \"RGBA\")\n\n# Mapping tá»« category_id sang tÃªn nhÃ£n\ncats = train_dataset.coco.cats\nid2label = {k: v['name'] for k, v in cats.items()}\n\n#if 0 not in id2label:\n    #id2label[0] = \"row\" \n\n# Váº½ bbox vÃ  nhÃ£n lÃªn áº£nh\nfor annotation in annotations:\n    box = annotation['bbox']  # bbox dáº¡ng [x, y, width, height]\n    class_idx = annotation['category_id']\n    \n    x, y, w, h = map(int, box)  # Chuyá»ƒn bbox sang sá»‘ nguyÃªn\n    draw.rectangle((x, y, x + w, y + h), outline='red', width=2)\n    \n    label_text = id2label.get(class_idx, f\"class_{class_idx}\")  # Náº¿u khÃ´ng tÃ¬m tháº¥y thÃ¬ dÃ¹ng tÃªn máº·c Ä‘á»‹nh\n    draw.text((x, y), label_text, fill='white')\n\n# Hiá»ƒn thá»‹ áº£nh káº¿t quáº£\ndisplay(image)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Láº¥y danh sÃ¡ch cÃ¡c ID áº£nh tá»« dataset COCO\nimage_ids = train_dataset.ids  # hoáº·c train_dataset.coco.getImgIds()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(id2label)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\ndef collate_fn(batch):\n  pixel_values = [item[0] for item in batch]\n  encoding = processor.pad(pixel_values, return_tensors=\"pt\")\n  labels = [item[1] for item in batch]\n  batch = {}\n  batch['pixel_values'] = encoding['pixel_values']\n  batch['pixel_mask'] = encoding['pixel_mask']\n  batch['labels'] = labels\n  return batch\n\ntrain_dataloader = DataLoader(train_dataset, collate_fn=collate_fn, batch_size=4, shuffle=True)\nval_dataloader = DataLoader(val_dataset, collate_fn=collate_fn, batch_size=2)\nbatch = next(iter(train_dataloader))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"batch.keys()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pixel_values, target = train_dataset[0]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pixel_values.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(target)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pytorch_lightning as pl\n# from transformers import DetrForObjectDetection\nfrom transformers import AutoModelForObjectDetection\nimport torch\n\nclass Detr(pl.LightningModule):\n     def __init__(self, lr, lr_backbone, weight_decay):\n         super().__init__()\n         # replace COCO classification head with custom head\n         # we specify the \"no_timm\" variant here to not rely on the timm library\n         # for the convolutional backbone\n        #  self.model = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\",\n        #                                                      revision=\"no_timm\",\n        #                                                      num_labels=len(id2label),\n        #                                                      ignore_mismatched_sizes=True)\n         self.model = AutoModelForObjectDetection.from_pretrained(\"microsoft/table-transformer-detection\",\n                                                                        # revision=\"no_timm\",\n                                                                        num_labels=len(id2label),\n                                                                        ignore_mismatched_sizes=True)\n         # see https://github.com/PyTorchLightning/pytorch-lightning/pull/1896\n         self.lr = lr\n         self.lr_backbone = lr_backbone\n         self.weight_decay = weight_decay\n\n     def forward(self, pixel_values, pixel_mask):\n       outputs = self.model(pixel_values=pixel_values, pixel_mask=pixel_mask)\n\n       return outputs\n\n     def common_step(self, batch, batch_idx):\n       pixel_values = batch[\"pixel_values\"]\n       pixel_mask = batch[\"pixel_mask\"]\n       labels = [{k: v.to(self.device) for k, v in t.items()} for t in batch[\"labels\"]]\n\n       outputs = self.model(pixel_values=pixel_values, pixel_mask=pixel_mask, labels=labels)\n\n       loss = outputs.loss\n       loss_dict = outputs.loss_dict\n\n       return loss, loss_dict\n\n     def training_step(self, batch, batch_idx):\n        loss, loss_dict = self.common_step(batch, batch_idx)\n        # logs metrics for each training_step,\n        # and the average across the epoch\n        self.log(\"training_loss\", loss)\n        for k,v in loss_dict.items():\n          self.log(\"train_\" + k, v.item())\n\n        return loss\n\n     def validation_step(self, batch, batch_idx):\n        loss, loss_dict = self.common_step(batch, batch_idx)\n        self.log(\"validation_loss\", loss)\n        for k,v in loss_dict.items():\n          self.log(\"validation_\" + k, v.item())\n\n        return loss\n\n     def configure_optimizers(self):\n        param_dicts = [\n              {\"params\": [p for n, p in self.named_parameters() if \"backbone\" not in n and p.requires_grad]},\n              {\n                  \"params\": [p for n, p in self.named_parameters() if \"backbone\" in n and p.requires_grad],\n                  \"lr\": self.lr_backbone,\n              },\n        ]\n        optimizer = torch.optim.AdamW(param_dicts, lr=self.lr,\n                                  weight_decay=self.weight_decay)\n\n        return optimizer\n\n     def train_dataloader(self):\n        return train_dataloader\n\n     def val_dataloader(self):\n        return val_dataloader","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = Detr(lr=1e-4, lr_backbone=1e-5, weight_decay=1e-4)\n\noutputs = model(pixel_values=batch['pixel_values'], pixel_mask=batch['pixel_mask'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"outputs.logits.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"learnable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f\"Sá»‘ tham sá»‘ há»c Ä‘Æ°á»£c: {learnable_params}/{total_params}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pytorch_lightning import Trainer\n\ntrainer = Trainer(max_epochs=200, gradient_clip_val=0.1, accelerator='gpu', devices=1)\ntrainer.fit(model)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.model.push_to_hub(\"10Ngoc/task02update\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pytorch_lightning import Trainer\nfrom pytorch_lightning.loggers import CSVLogger\nimport pandas as pd\nimport matplotlib.pyplot as plt","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom pytorch_lightning import Trainer\nfrom pytorch_lightning.loggers import CSVLogger\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Táº¡o mÃ´ hÃ¬nh vá»›i cÃ¡c siÃªu tham sá»‘\nmodel = Detr(lr=1e-4, lr_backbone=1e-5, weight_decay=1e-4)\noutputs = model(pixel_values=batch['pixel_values'], pixel_mask=batch['pixel_mask'])\n\n# Táº¡o logger Ä‘á»ƒ lÆ°u loss má»—i epoch\nlogger = CSVLogger(save_dir=\"logs/\", name=\"detr_structure_recognition\")\n\n# Cáº¥u hÃ¬nh Trainer\ntrainer = Trainer(\n    max_epochs=10,\n    logger=logger,\n    accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n    log_every_n_steps=10,\n)\n\n# Huáº¥n luyá»‡n mÃ´ hÃ¬nh\ntrainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n\n# Äá»c log CSV Ä‘á»ƒ váº½ biá»ƒu Ä‘á»“\nlog_path = logger.log_dir + \"/metrics.csv\"\ndf = pd.read_csv(log_path)\n\n# Váº½ biá»ƒu Ä‘á»“ loss\nplt.plot(df[\"epoch\"], df[\"training_loss\"], label=\"Train Loss\")\nif \"validation_loss\" in df.columns:\n    plt.plot(df[\"epoch\"], df[\"validation_loss\"], label=\"Val Loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.title(\"Training & Validation Loss\")\nplt.legend()\nplt.grid(True)\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoImageProcessor, AutoModelForObjectDetection, TableTransformerForObjectDetection\nimport torch\n\nmodel = TableTransformerForObjectDetection.from_pretrained(\"10Ngoc/task01tabledetection\", id2label={0:\"table\"})\n# model = TableTransformerForObjectDetection.from_pretrained(\"microsoft/table-transformer-detection\",\n#                                                                         revision=\"no_timm\",\n#                                                                         num_labels=len(id2label),\n#                                                                         ignore_mismatched_sizes=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q coco-eval","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def convert_to_xywh(boxes):\n    xmin, ymin, xmax, ymax = boxes.unbind(1)\n    return torch.stack((xmin, ymin, xmax - xmin, ymax - ymin), dim=1)\n\ndef prepare_for_coco_detection(predictions):\n    coco_results = []\n    for original_id, prediction in predictions.items():\n        if len(prediction) == 0:\n            continue\n\n        boxes = prediction[\"boxes\"]\n        boxes = convert_to_xywh(boxes).tolist()\n        scores = prediction[\"scores\"].tolist()\n        labels = prediction[\"labels\"].tolist()\n\n        coco_results.extend(\n            [\n                {\n                    \"image_id\": original_id,\n                    \"category_id\": labels[k],\n                    \"bbox\": box,\n                    \"score\": scores[k],\n                }\n                for k, box in enumerate(boxes)\n            ]\n        )\n    return coco_results","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from coco_eval import CocoEvaluator\nfrom tqdm.notebook import tqdm\n\nimport numpy as np\n\n# initialize evaluator with ground truth (gt)\nevaluator = CocoEvaluator(coco_gt=train_dataset.coco, iou_types=[\"bbox\"])\n\nprint(\"Running evaluation...\")\nfor idx, batch in enumerate(tqdm(train_dataloader)):\n    # get the inputs\n    pixel_values = batch[\"pixel_values\"].to(device)\n    pixel_mask = batch[\"pixel_mask\"].to(device)\n    labels = [{k: v.to(device) for k, v in t.items()} for t in batch[\"labels\"]] # these are in DETR format, resized + normalized\n\n    # forward pass\n    with torch.no_grad():\n      outputs = model(pixel_values=pixel_values, pixel_mask=pixel_mask)\n\n    # turn into a list of dictionaries (one item for each example in the batch)\n    orig_target_sizes = torch.stack([target[\"orig_size\"] for target in labels], dim=0)\n    results = processor.post_process_object_detection(outputs, target_sizes=orig_target_sizes, threshold=0)\n\n    # provide to metric\n    # metric expects a list of dictionaries, each item\n    # containing image_id, category_id, bbox and score keys\n    predictions = {target['image_id'].item(): output for target, output in zip(labels, results)}\n    predictions = prepare_for_coco_detection(predictions)\n    evaluator.update(predictions)\n\nevaluator.synchronize_between_processes()\nevaluator.accumulate()\nevaluator.summarize()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import requests\nfrom io import BytesIO\nfrom PIL import Image\n\nurl = \"https://www.dropbox.com/scl/fi/glyymn5opvhmd929004ri/28_1.jpg?rlkey=d99hbhszy1z922ywei315bl06&st=ewtu61ki&dl=1\"\n\n# Dropbox link gá»‘c khi táº£i trá»±c tiáº¿p, thÆ°á»ng báº¡n pháº£i Ä‘á»•i 'dl=0' thÃ nh 'dl=1' hoáº·c 'raw=1' Ä‘á»ƒ cÃ³ link trá»±c tiáº¿p\n# á»ž Ä‘Ã¢y dl=1 rá»“i nÃªn táº£i trá»±c tiáº¿p Ä‘Æ°á»£c.\n\nresponse = requests.get(url)\nimage = Image.open(BytesIO(response.content)).convert(\"RGB\")\n\n# Tiáº¿p theo, nhÆ° báº¡n Ä‘Ã£ lÃ m:\ninputs = processor(images=image, return_tensors=\"pt\").to(device)\n\nwith torch.no_grad():\n    outputs = model(**inputs)\n\ntarget_sizes = torch.tensor([image.size[::-1]]).to(device)\nresults = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.5)[0]\n\npredictions = [{\n    \"image_id\": 0,\n    \"category_id\": label.item(),\n    \"bbox\": box.tolist(),\n    \"score\": score.item()\n} for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"])]\n\n\nfrom PIL import Image\nimport itertools, math\nimport matplotlib.pyplot as plt\n\n# ------------------------------------------------\n# 0. ThÃ´ng sá»‘\n# ------------------------------------------------\nPAD = 1                            # sá»‘ pixel padding quanh má»—i cell\n\n# ------------------------------------------------\n# 1. PhÃ¢n loáº¡i bbox hÃ ng / cá»™t\n# ------------------------------------------------\nrows = [tuple(map(int, p[\"bbox\"])) for p in predictions if p[\"category_id\"] == 0]\ncols = [tuple(map(int, p[\"bbox\"])) for p in predictions if p[\"category_id\"] == 1]\n\n# ------------------------------------------------\n# 2. Giao nhau -> box Ã´\n# ------------------------------------------------\ndef intersect(a, b):\n    x0, y0 = max(a[0], b[0]), max(a[1], b[1])\n    x1, y1 = min(a[2], b[2]), min(a[3], b[3])\n    return (x0, y0, x1, y1) if x1 > x0 and y1 > y0 else None\n\ncells = [intersect(r, c) for r, c in itertools.product(rows, cols)]\ncells = [b for b in cells if b]                     # loáº¡i Ã´ áº£o\ncells.sort(key=lambda b: (b[1], b[0]))              # Ä‘á»c trÃªn-xuá»‘ng, trÃ¡i-pháº£i\n\n# ------------------------------------------------\n# 3. HÃ m pad & cáº¯t\n# ------------------------------------------------\nW, H = image.size\ndef pad_box(b, p=PAD):\n    x0 = max(0, b[0] - p)\n    y0 = max(0, b[1] - p)\n    x1 = min(W, b[2] + p)\n    y1 = min(H, b[3] + p)\n    return (x0, y0, x1, y1)\n\ncell_imgs = [image.crop(pad_box(b)) for b in cells]\n\n# ------------------------------------------------\n# 4. Hiá»ƒn thá»‹ lÆ°á»›i (tÃ¹y chá»n)\n# ------------------------------------------------\nn = len(cell_imgs)\ncols_grid = min(6, n)\nrows_grid = math.ceil(n / cols_grid)\nfig, axes = plt.subplots(rows_grid, cols_grid, figsize=(2.5*cols_grid, 2.5*rows_grid))\n\nfor ax in axes.flat:\n    ax.axis(\"off\")\nfor i, im in enumerate(cell_imgs):\n    ax = axes.flat[i]\n    ax.imshow(im)\n    ax.set_title(f\"Cell {i}\", fontsize=8)\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import requests, zipfile, io, os, itertools, math, shutil\nfrom pathlib import Path\nfrom PIL import Image, ImageOps\nimport torch\n\n\n# xÃ³a ZIP cÅ© náº¿u cÃ³\nif Path(OUTPUT_ZIP).exists():\n    Path(OUTPUT_ZIP).unlink()\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# CONFIGURATION (customize chá»— nÃ y nha)\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nZIP_URL       = \"https://www.dropbox.com/scl  /fo/ovyizztnavkpf3929nu2g/AId_MJ7X-E4XcV96jMIT3yI?rlkey=6i37jm6nr4jsx5z3d2f1fwsze&dl=1\"\nTMP_DIR       = Path(\"ZIP_EXTR\")\nOUT_ROOT      = Path(\"OUT_CELLS\")\nPAD           = 1                  # pixel padding around each cell crop\nDEVICE        = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n# dá»n sáº¡ch má»i thá»© cá»§a láº§n cháº¡y trÆ°á»›c\nfor p in (TMP_DIR, OUT_ROOT):\n    if p.exists():\n        shutil.rmtree(p)\n    p.mkdir(parents=True, exist_ok=True)\n\n# resize rules\nSQUARE_SIZE   = (512, 512)\nRECT_SIZE     = (640, 480)\nASPECT_TOLER  = 0.05               # |w/h âˆ’ 1| â‰¤ tol â†’ treat as square\nKEEP_ASPECT   = False # Stretch the image\nPAD_COLOR     = (255, 255, 255)    # for letter-box\n\n# name of the final zip you will download\nOUTPUT_ZIP    = \"OUT_CELLS.zip\"\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# HOUSEKEEPING  (clean old runs)\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nshutil.rmtree(TMP_DIR,  ignore_errors=True)\nshutil.rmtree(OUT_ROOT, ignore_errors=True)\nfor p in (TMP_DIR, OUT_ROOT):\n    p.mkdir(parents=True, exist_ok=True)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# 1. DOWNLOAD ZIP\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nprint(\"â¬‡ï¸  Downloading ZIP â€¦\")\nresp = requests.get(ZIP_URL, stream=True)\nresp.raise_for_status()\nzip_bytes = io.BytesIO(resp.content)\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# 2. EXTRACT JPGs\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nprint(\"ðŸ“¦ Extracting JPGs â€¦\")\nwith zipfile.ZipFile(zip_bytes) as zf:\n    for member in zf.namelist():\n        if member.lower().endswith(\".jpg\"):\n            zf.extract(member, path=TMP_DIR)\n\njpg_paths = sorted(TMP_DIR.rglob(\"*.jpg\"))\nprint(f\"âœ… Found {len(jpg_paths)} images\\n\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Utils functions","metadata":{}},{"cell_type":"code","source":"def intersect(a, b):\n    x0, y0 = max(a[0], b[0]), max(a[1], b[1])\n    x1, y1 = min(a[2], b[2]), min(a[3], b[3])\n    return (x0, y0, x1, y1) if x1 > x0 and y1 > y0 else None\n\ndef pad_box(b, p, w, h):\n    x0 = max(0, b[0]-p); y0 = max(0, b[1]-p)\n    x1 = min(w, b[2]+p); y1 = min(h, b[3]+p)\n    return (x0, y0, x1, y1)\n\ndef resize_with_letterbox(img, target, keep_aspect=True, fill=PAD_COLOR):\n    \"\"\"Resize to *target* (w,h).  If keep_aspectâ†’True, letter-box pad.\"\"\"\n    if not keep_aspect:\n        return img.resize(target, Image.LANCZOS)\n\n\n    tw, th = target\n    iw, ih = img.size\n    scale  = min(tw/iw, th/ih)\n    nw, nh = int(iw*scale), int(ih*scale)\n    img_r  = img.resize((nw, nh), Image.LANCZOS)\n    canvas = Image.new(\"RGB\", target, fill)\n    canvas.paste(img_r, ((tw-nw)//2, (th-nh)//2))\n    return canvas\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Process each image","metadata":{}},{"cell_type":"code","source":"\nfor img_path in jpg_paths:\n    image = Image.open(img_path).convert(\"RGB\")\n\n    # â”€â”€ Run detector â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    inputs = processor(images=image, return_tensors=\"pt\").to(DEVICE)\n    with torch.no_grad():\n        outputs = model(**inputs)\n\n    target_sizes = torch.tensor([image.size[::-1]]).to(DEVICE)\n    results = processor.post_process_object_detection(\n                    outputs, target_sizes=target_sizes, threshold=0.5)[0]\n\n    predictions = [{\n        \"category_id\": label.item(),\n        \"bbox\": list(map(int, box.tolist()))\n    } for label, box in zip(results[\"labels\"], results[\"boxes\"])]\n\n    rows = [p[\"bbox\"] for p in predictions if p[\"category_id\"] == 0]\n    cols = [p[\"bbox\"] for p in predictions if p[\"category_id\"] == 1]\n\n    cell_boxes = [intersect(r, c) for r, c in itertools.product(rows, cols)]\n    cell_boxes = [b for b in cell_boxes if b]\n    cell_boxes.sort(key=lambda b: (b[1], b[0]))   # reading order\n\n    # â”€â”€ Crop, resize, save â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    W, H = image.size\n    out_dir = OUT_ROOT / img_path.stem\n    out_dir.mkdir(parents=True, exist_ok=True)\n\n    for idx, b in enumerate(cell_boxes):\n        pb   = pad_box(b, PAD, W, H)\n        crop = image.crop(pb)\n\n        # choose target size\n        w, h      = crop.size\n        is_square = abs((w/h) - 1.0) <= ASPECT_TOLER\n        target_sz = SQUARE_SIZE if is_square else RECT_SIZE\n        \n        final = resize_with_letterbox(\n                    crop,\n                    target=target_sz,\n                    keep_aspect=False,        # <â”€â”€ táº¯t giá»¯ tá»‰ lá»‡\n                    fill=PAD_COLOR            # giÃ¡ trá»‹ nÃ y sáº½ bá»‹ bá» qua khi keep_aspect=False\n                )\n        final.save(out_dir / f\"cell_{idx:03d}.jpg\", quality=95)\n\n    print(f\"{img_path.name:35s} âžœ  {len(cell_boxes):3d} cells\")\n\nprint(\"\\nðŸ¥³  All done! Crops live in:\", OUT_ROOT.resolve())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Make zip file","metadata":{}},{"cell_type":"code","source":"\nif os.path.exists(OUTPUT_ZIP):\n    os.remove(OUTPUT_ZIP)\n\nshutil.make_archive(OUTPUT_ZIP.replace(\".zip\",\"\"), \"zip\", root_dir=OUT_ROOT)\nprint(f\"ðŸ“  Created {OUTPUT_ZIP} â€“ grab it from the right-hand Files pane.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import FileLink, display\ndisplay(FileLink(OUTPUT_ZIP))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}